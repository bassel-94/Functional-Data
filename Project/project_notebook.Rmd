---
title: |
  | \vspace{8cm} \textbf{Wheat moisture prediction using functional data analysis}
author:
- Bassel MASRI
- Cyril DEVEDEUX
date: "2/14/2021"
output:
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(dev = 'png')     #-- to reduce the size of the knitted pdf
```

\newpage
\tableofcontents
\newpage

# Introduction

Commercial wheat products such as flower are sold on a weight basis. The grains, however, contain a certain amount of water (i.e. moisture) on which there are restrictions about what moisture contents are allowed. Such restrictions have become a useful standard of identity for grain and flour to make sure that purchasers are buying what they expect. Mastering the level of moisture in wheat grains would highly affect its lifespan as well as how long it could be stored. According to a study on grain moisture [ref.1], the ideal moisture of wheat should be between 13% and 17%  On the other hand, determining the level of moisture requires expensive lab equipment and a significant time to obtain accurate results. To overcome such expenses, some data-driven methods approaches have been developed to predict the flour quality parameters from near infrared reflectance (NIR) spectroscopy of the wheat grains [ref.2] through neural networks. Our main contribution throughout this study is to use a functional data approach to determine the level of moisture based on the NIR spectroscopy of the wheat grains.

# The problem and the data

## Defining the research question

The spectral curves of wheat grains is easy to obtain, while chemical analysis to determine the level of any molecular structure of the wheat (including moisture) is time consuming and expensive. Therefore, to solve such problem, we will take a functional-data driven approach to regress on the moisture level of wheat grains using their equivalent NIR spectroscopy through a **scalar-on-functional regression analysis**.
The purpose of regression analysis is to determine the form of dependence between the moisture level (i.e. the target variable $Y$) and the spectral curve (i.e. the functions $X$).

Mathematically, we would like to find a functional $g$ such that 

$$
g : L^2 \rightarrow \mathbb{R} \text{ such that } Y = g(X)
$$

Once the regression function is defined, it becomes easy to determine an approximate moisture level of a wheat grain sample from its spectral curve.

## The data

The *Moisturespectrum* data included in the `R` package `fds` is a data set that consists of near-infrared reflectance spectra of 100 wheat samples, measured in $2$ nm intervals from $1100$to $2500$nm. Their associated response variable, the samplesâ€™ moisture content which is a scalar, is included in a different dataset called *Moisturevalues*.

```{r, include=FALSE}
#-- load libraries
rm(list=ls())
library(fda)
library(refund)
library(tidyverse)
if (!require(fds)) install.packages("fds")
suppressPackageStartupMessages(library(fda))
```

```{r data, fig.width=10, fig.height=4, fig.cap="\\label{fig:data}NIR spectroscopy of 100 wheat samples (on the left) and their moisture levels (on the right)"}
#-- load data and response variable
data("Moisturespectrum")
data("Moisturevalues")
Y = Moisturevalues
X = Moisturespectrum

#-- plot the data
par(mfrow=c(1,2))
plot(X, col = "grey", cex.lab = 0.7, cex.axis = 0.7)
plot(Y, xlab="Sample",
     ylab="Moisture level in %", 
     cex = 0.7,
     cex.lab = 0.7, 
     cex.axis = 0.7,
     pch=15)
```

```{r}
to_print = X$y[1:5, 1:5]
knitr::kable(to_print, align = 'c', 
             col.names = c("sample 1", "sample 2", "sample 3", "sample 4", "sample 5"),
             caption = "Table showing the first few rows and columns of the NIR spectra\\label{tab:sample}")
```

Figure \ref{fig:data} shows the spectral curves of 100 wheat samples densely packed together on the left, and their corresponding moisture level in percentage on the right. Table \ref{tab:sample} displays the first few rows and columns of the NIR spectra values. The row names of the matrix represent the wavelength in nano-meters and the column names represent the sample number.

# Approach and results

In this section, we will go through the details of our analysis steps we took in order to solve the problem. We start with an exploratory analysis, then move on to the modeling aspect along with the results and discussions. 

## Exploratory data analysis

As with all statistical modeling, data exploration is always the first step prior to any model development. Therefore, our analysis begins with a suitable exploratory analysis for functional data where we investigate a suitable basis function to smooth the data then compute the point wise statistics such as mean, standard deviation and their 95% confidence intervals.

### Manual smoothing

The first task is to smooth the functions using a basis. The data do not show any periodicity as we can see in Figure \ref{fig:data}. 
Therefore, the optimal basis function would be B splines. At first, we choose 25 basis and explore the plot produced in Figure \ref{fig:smooth}. We add to the latter the mean function (the black curve) as well as the standard deviation plotted in red. We also compute and plot the 95% confidence interval in dashed green.

```{r, include=FALSE}
mat = X$y
#dim(mat)

N = dim(mat)[1] # 701
M = dim(mat)[2] #100

#-- set number of basis and generate it
B25.basis = create.bspline.basis(rangeval = c(1,M), norder = 4, nbasis = 25)

#-- plot the generated basis
mat_fd = Data2fd(1:M,t(mat),B25.basis)
```

```{r smooth, fig.width=5, fig.height=3.5, fig.cap="\\label{fig:smooth}Smoothed curves using 25 B splines", fig.align='center', message=FALSE}
# Pointwise mean
muhat <- mean.fd(mat_fd)

# Standard deviation
sdhat <- sd.fd(mat_fd)

# Confidence interval 95 %
invisible(plot(mat_fd, lty = 1, ylim=c(0,1.5),col = "grey",cex.lab = 0.7,cex.axis = 0.7))
lines(muhat,lwd = 2, col = "black")
lines(muhat+1.96*sdhat,lwd=1,lty = 2,col = "green")
lines(muhat-1.96*sdhat,lwd=1,lty = 2,col = "green")
#lines(sdhat,lwd = 4, col = "red")
legend(80,1.55,c("mean","95% CI"), col = c("black","green"), lwd=2, cex = 0.7)
```

Interestingly, the smoothed data follows the classic two standard deviation rule surprisingly well with nearly all of the curves falling between the green lines. Figure \ref{fig:sd} shows the point-wise sample standard deviation which gives us an idea about the variability of curves at any point $t$. Indeed, we notice more variability in the early samples than in the last samples. This may be explained by the fact that the moisture level in those samples are quite different.

```{r sd, fig.width=5, fig.height=3.5, fig.cap="\\label{fig:sd}Standard deviation of the curves", fig.align='center'}
invisible(plot(sdhat,lwd = 2, col = "red",cex.lab = 0.7,cex.axis = 0.7))
```

The point-wise sample standard deviation gives no information on how the values of the curves at point $t$ relate to those at point $s$. Therefore we compute the sample covariance function $\hat{c}(t,s)$ and we plot its perspective and its contour plot as seen in Figure \ref{fig:cov} and Figure \ref{fig:cont}

```{r cov, fig.width=6, fig.height=5, fig.cap="\\label{fig:cov}Perspective plot of the covariance function", fig.align='center'}
Chat <- var.fd(mat_fd)
#class(Chat) # we check that it's a bivariate functional data object.
eval_times<-seq(1,100,length=90)
Chat_matrix<-eval.bifd(eval_times,eval_times,Chat)

persp(eval_times,eval_times,Chat_matrix, 
      theta = -45,phi = 30, ticktype = "detailed", 
      expand = 0.5, cex.lab = 0.7, cex.axis = 0.5, 
      xlab="s", ylab="t", zlab="c(s,t)")
```

```{r cont, fig.width=4, fig.height=4, fig.cap="\\label{fig:cont}Contour plot of the covariance function", fig.align='center'}
library(plotrix)
contour(eval_times, eval_times, Chat_matrix, 
        lwd=2, cex.lab = 0.7,cex.axis = 0.7)
draw.circle(10,10,10, border = "red", lty = 3, lwd = 2)
```

The 3D perspective plot shows that the variation in the spectrum is higher in the first samples. Indeed, this is confirmed in the contour plot in Figure \ref{fig:cont} in the highlighted red circle.

### Penalized smoothing

Typically, when the raw data curves exhibit a substantial level of noise, the functional objects constructed using manual basis expansion smoothing (i.e $M=25$ in our case) will inherit this variability, and thus resulting in *wiggly* curves. To avoid amplifying said variability, we will perform smoothing using a penalized approach.

To choose the tuning parameter $\lambda$, generalized cross-validation (GCV) is employed. The aim is to minimize the penalized sum of squares with respect to the tuning parameter $\lambda$.

```{r gcv, fig.width=5, fig.height=3.5, fig.cap="\\label{fig:gcv}Values of the tuning parameter using generalized cross validation", fig.align='center'}
pen_basis = create.bspline.basis(c(1,M),nbasis=150,norder=4)
loglam = seq(-1,4,by=0.2)
nlam = length(loglam)

dfsave = numeric(nlam); names(dfsave) = loglam
gcvsave = numeric(nlam); names(gcvsave) = loglam

for (ilam in 1:nlam) {
	#cat(paste('log10 lambda =',loglam[ilam],'\n'))
	lambda = 10^loglam[ilam]
	fdParobj = fdPar(fdobj = pen_basis, Lfdobj = int2Lfd(2), lambda=lambda)
	smoothlist = smooth.basis(y=t(mat),fdParobj=fdParobj) 
	dfsave[ilam] = smoothlist$df
	gcvsave[ilam] = sum(smoothlist$gcv) 
}

plot(loglam,gcvsave,type='b', lwd=1.5, 
     ylab = "generalized CV error", 
     xlab = "log lambda", cex.lab = 0.7,
     cex.axis = 0.7, cex = 0.5)
```

Generalized cross validation shows an optimal smoothing parameter $\lambda = 39.8$ when choosing a range for $\lambda$ between $[10^{-1}, 10^4]$ and a number of basis of $150$ knowing that we only have $100$ samples. The cross validation curve with respect to the values of $\lambda$ is shown in Figure \ref{fig:gcv}. Using the minimum value of $\lambda$, we penalize the basis expansion and repeat the smoothing task which yields the curves in the bottom plot in Figure \ref{fig:pen}.

```{r pen, fig.width=8, fig.height=3.5, fig.cap="\\label{fig:pen}Smoothed curves without penalization (right) vs with penalization (left)", fig.align='center'}
optilambda<-10^(loglam[which.min(gcvsave)])
fdParobj = fdPar(fdobj = pen_basis, Lfdobj = int2Lfd(2),lambda=optilambda)
result.fd = smooth.basis(y=t(mat),fdParobj=fdParobj)$fd

# Pointwise mean
mean.result.fd = mean.fd(result.fd)

# Standard deviation
sd.result.fd = std.fd(result.fd)

par(mfrow = c(1,2))

#-- plot with penalization
invisible(plot(result.fd, col = "grey", lty = 1, cex.lab = 0.7, cex.axis = 0.7, ylim = c(0.3, 1.25)))
lines(mean.result.fd,lwd = 2, col = "black")
lines(mean.result.fd+1.96*sd.result.fd,lwd=2,lty = 2,col = "green")
lines(mean.result.fd-1.96*sd.result.fd,lwd=2,lty = 2,col = "green")

#-- plot without penalization
invisible(plot(mat_fd, lty = 1, ylim = c(0.3, 1.25),col = "grey",cex.lab = 0.7,cex.axis = 0.7))
lines(muhat,lwd = 2, col = "black")
lines(muhat+1.96*sdhat,lwd=1,lty = 2,col = "green")
lines(muhat-1.96*sdhat,lwd=1,lty = 2, col = "green")
```

Indeed, the results indicate that penalizing the smoothed basis reduces unnecessary variance significantly. The mean curve of the penalized smoothing seen in the top plot of Figure \ref{fig:pen} is much less noisy, showing peaks where the relative difference between the curves actually matters, and a smooth plateau where the difference is insignificant.

### Functional principal components analysis

One of the most useful tools in functional data analysis is the principal component analysis. Estimated functional principal components, EFPCâ€™s, are related to the sample covariance function $\hat{c}(t,s)$. Similar to usual multivariate statistics, we try to reduce the dimensionality of the feature space using only a few functions $\hat{\nu}_j$ such that the centered functions $X_n - \bar{X}_N$ are represented as follows :

$$
X_n(t) - \bar{X}_N(t) \approx \sum_{j=1}^p \xi_{nj} \hat{\nu}_j(t)
$$

Where $p$ is a much smaller dimension than $M$ the number of basis. Note that $\xi_{nj}$ are the scores of the components. The principal component analysis will be applied on the functions produced after penalized smoothing observed in the top plot of Figure \ref{fig:pen}.

```{r pca, fig.align="center", fig.width=5, fig.height=3.5, fig.cap="\\label{fig:pen}The percentage of variability explained vs FPCs on smoothed curves using penalization"}
#-- compute fpc
result.fd.PCA = pca.fd(fdobj = result.fd, nharm = 5)
mat_fd.PCA = pca.fd(fdobj = mat_fd, nharm = 5)

#-- get cumulative variability
c = cumsum(result.fd.PCA$varprop)
plot(c, type = "b", lwd = 1.5, ylim = c(0.999,1),
     col = "steelblue", 
     xlab = "Number of PCs", 
     ylab = "% var. explained",
     cex = 0.5, cex.lab = 0.7, cex.axis = 0.7)
```

```{r}
to_print = t(c)
row.names(to_print) = "Cumulated Var."
knitr::kable(to_print, col.names = c("FPC1", "FPC2", "FPC3", "FPC4", "FPC5"), align = 'c', caption = "Table showing the cumulated variance of the first 5 PC\\label{tab:pctab}")
```

Figure \ref{fig:pca} and Table \ref{tab:pctab} indicate that we are able to capture more than $99$% of the variability using only the first principal component. This is not surprising due to the fact that the plots are very similar and densely packed together as we can observe in the Figure \ref{fig:data}. 

```{r harm, fig.align="center", fig.width=4, fig.height=3.5, fig.cap="\\label{fig:harm}plot of the first three FPCs"}
invisible(plot(result.fd.PCA$harmonics[1:3], lwd =1.5, cex.lab = 0.7, cex.axis = 0.7))
```

We visualize the plot of the first 3 components in Figure \ref{fig:harm}. The first principal component (in black line) indicates that almost all samples' variance is captured equivalently. This is coherent with our previous findings regarding curve similarities. There will be no need to further explore the curves for the next steps.
\newpage

## Modeling

### Model definition and evaluation

Functional regression models can be identified using three main categories based on the nature of the target variable and the predictors. These categories are; scalar-on-function, function-on-scalar and function-on-function. Our exploratory data analysis has shown that the response variable is a scalar (i.e. a number on $\mathbb{R}$), which makes a scalar-on-function regression analysis an ideal candidate to solve the problem at hand [ref.3]. The regression model is defined as follows: 

$$
Y_i = \int \beta(s)X_i(s)ds + \varepsilon_i
$$
In order to evaluate the model, we will use the $R^2$ as our main metric. Further more, we will display the diagnostic plots and analyze them to further confirm (or reject) the model.

### Functional principal components regression

We have seen in the previous section that it is possible to capture a high level of variance using only two functional principal components. Naturally, the easiest regression model we could fit is a Penalized Functional Regression (pfr) model on the principal components. We consider all 100 observations as our training set and use the `R` command pfr to regress $Y$ on `fpc(X)` setting the number of components to 4.

```{r, include=FALSE}
#-- reload data and do test train split
Y = Moisturevalues
X = t(Moisturespectrum$y)

#-- fit model
model.1 = pfr(Y~fpc(X, ncomp = 4))
cat("R squared score is",summary(model.1)$r.sq)
```

Upon fitting the model using the above-mentioned settings, we obtain a high 97.33% $R^2$ score. 
Next, we produce a plot of the fitted coefficient along with its $95$% confidence interval in the Figure below.

```{r d, fig.width=5, fig.height=3.5, fig.align='center'}
#-- get conf interval and coef
model.fpcr.se = coef(model.1,n=dim(X)[2])$se
model.fpcr.coef = coef(model.1,n=dim(X)[2])$value

#-- get 95% confidence interval for the coef
IC_max = model.fpcr.coef+1.96*model.fpcr.se
IC_min = model.fpcr.coef-1.96*model.fpcr.se

#-- plot results
grid=seq(from = 1100, to = 2500, length.out = dim(X)[2])
plot(grid,model.fpcr.coef,type='l',
     col='steelblue',xlab="Wavelength",
     ylab="value", cex.lab = 0.7, cex.axis = 0.7)
lines(grid,IC_max,lty=2,col='darkred')
lines(grid,IC_min,lty=2,col='darkred')
```

The coefficient plot, however, does not give much insight on how well the model predicts values. Therefore, we produce a plot of fitted against observed values to visualize the linear trend and get a better idea on the goodness of fit. In addition, we plot the residuals to make sure that there is no structure in its distribution and that it is centered around $0$.

```{r diag, fig.align="center", fig.width=7, fig.height=3, fig.cap="\\label{fig:diag}plot of residuals(left) and observed vs. predicted values (right)"}
par(mfrow = c(1,2))
plot(model.1$residuals, cex.lab = 0.7, cex.axis = 0.7, cex = 0.6, xlab = "Sample", ylab = "value")
abline(h=0, col = "darkred")

Y_hat = model.1$fitted.values
plot(x = Y_hat, y=Y, cex.lab = 0.7, cex.axis = 0.7, cex = 0.6, xlab = "Predicted values", ylab = "Observed values")
abline(c(0,1), col = "darkred")
```

The two diagnostic plots produced in Figure \ref{fig:diag} show a nice linear trend between predicted and observed percentages of moisture in the wheat grains. The residuals plot shows a random structure and centered around $0$ which further confirms a good fit.

### Regression using basis expansion approach

Regression using basis expansion approach works is by expanding the function $\beta$ using deterministic basis functions. This allows us to rewrite it as follows :

$$
\hat{\beta}(t) = \sum_{k=1}^K \hat{c}_k B_k(t)
$$

Then, we could integrate it in the original regression equation defined above to solve the problem. One disadvantage of such approach is that the basis functions $B_k$ and their number $K$ are highly subjective to the user. In our case, since we used a B-spline basis function as an expansion of the regressor functions, we will do the same here. For $k=15$, we get an $R^2$ score of $97.66$ which is highly similar to the one produced by the principal components regression. Interestingly, the plot below shows a significantly different coefficient curve compared to the one produced before. The confidence intervals are larger and the overall "roughness" of the curve is higher. This, again, is highly dependent on the user's choice of basis functions and their number.

```{r, include=FALSE}
#-- fit model
model.2 = pfr(formula = Y~lf(X, bs="cr", k=15, fx = TRUE))

#-- computing R squared
cat("R squared score is",summary(model.2)$r.sq)
```

```{r c, fig.width=5, fig.height=3.5, fig.align='center'}
#-- get 95% confidence interval for the coef
model.bpcr.se=coef(model.2,n=dim(X)[2])$se
model.bpcr.coef=coef(model.2,n=dim(X)[2])$value

#-- Definition of coefficient intervals
IC_max=model.bpcr.coef+1.96*model.bpcr.se
IC_min=model.bpcr.coef-1.96*model.bpcr.se

#-- plot results
plot(grid,model.bpcr.coef,type='l',col='steelblue',
     ylim=c(-5000,5000),xlab="Wavelength", ylab="value", 
     cex.lab = 0.7, cex.axis = 0.7)
lines(grid,IC_max,lty=2,col='darkred')
lines(grid,IC_min,lty=2,col='darkred')
```

To compare the results with the previous section, we produce the same residuals plot and the observed vs. predicted values plot as seen in Figure \ref{fig:diag2}.

```{r diag2, fig.align="center", fig.width=7, fig.height=3, fig.cap="\\label{fig:diag2}plot of residuals(left) and observed vs. predicted values (right)"}
par(mfrow = c(1,2))
plot(model.2$residuals, cex.lab = 0.7, cex.axis = 0.7, cex = 0.6, xlab = "Sample", ylab = "value")
abline(h=0, col = "darkred")

Y_hat = model.2$fitted.values
plot(x = Y_hat, y=Y, cex.lab = 0.7, cex.axis = 0.7, cex = 0.6, xlab = "Predicted values", ylab = "Observed values")
abline(c(0,1), col = "darkred")
```

# Conclusion

A dimensionality reduction approach has always been useful to solve high dimensional multivariate data by projecting it onto a smaller, more manageable feature space. From a functional data approach, this has proven to give accurate results when solving a scalar-on-function regression to solve the problem in this study which is to be able to detect the level of moisture in wheat grains from its NIR spectra. Indeed, based on its $R^2$ estimate, the problem at hand has been solved which might help reduce the cost of chemical analyses in future works. On the other hand, a regression using basis expansion approach is more easily interpretable based on the fact that we do not reduce the dimension of the data. The downside, however, is that a careful choice of the basis expansion functions and their number has to be taken.

Throughout this study, we have considered only the $R^2$ as our primary model selection metric. Future works could explore different (if not multiple) evaluation metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). In addition, the evaluation has been done on the same data we used to fit the model which makes it a biased estimator. In order to obtain an unbiased metric, one would have to perform a train-validation split which was beyond the scope of this study. 

# References

[ref.1] Grain moisture â€“ guidelines for measurement
link https://projectblue.blob.core.windows.net/media/Default/Imported%20Publication%20Docs/Grain%20moisture%20%E2%80%93%20guidelines%20for%20measurement.pdf

[ref.2] Prediction of wheat quality parameters using near-infrared spectroscopy and artificial neural networks
link https://www.researchgate.net/publication/227299821_Prediction_of_wheat_quality_parameters_using_near-infrared_spectroscopy_and_artificial_neural_networks

[ref.3] Kokoszka, P. and Reimherr, M. (2017). Introduction to Functional Data Analysis. Chapman and Hall/CRC.
