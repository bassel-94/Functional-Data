---
title: |
  | \vspace{8cm} \textbf{Wheat moisture prediction using functional data analysis}
author:
- Bassel MASRI
- Cyril DEVEDEUX
date: "2/14/2021"
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dev = 'png')     #-- to reduce the size of the knitted pdf
```

\newpage
\tableofcontents
\newpage

# Introduction

Commercial wheat products such as flower are sold on a weight basis. The grains, however, contain a certain amount of water (i.e. moisture) on which there are restrictions about what moisture contents are allowed. Such restrictions have become a useful standard of identity for grain and flour to make sure that purchasers are buying what they expect. Mastering the level of moisture in wheat grains would highly affect its lifespam as well as how long it could be stored. According to a study on grain moisture [ref.1], the ideal moisutre of wheat should be between 13% and 17%  On the other hand, determining the level of moisture requires expensive lab equipement and a significant time to obtain accurate results. To overcome such expenses, some data-driven methods approaches have been developed to predict the flour quality parameters from near infrared reflectance (NIR) spectroscopy of the wheat grains [ref.2] through neural networks. Our main contribution throughout this study is to use a functional data approach to determine the level of moisture based on the NIR spectroscopy of the wheat grains.

# The problem and the data

## Defining the research question

The spectral curves of wheat grains is easy to obtain, while chemical analysis to determine the level of any molecular structure of the wheat (including moisutre) is time consuming and expensive. Therefore, to solve such problem, we will take a functional-data driven approach to regress on the moisture level of wheat grains using their equivalent NIR spectroscopy through a **scalar-on-functional regression analysis**.
The purpose of regression analysis is to determine the form of dependence between the moisture level (i.e. the target variable $Y$) and the spectral curve (i.e. the functions $X$).

Mathematically, we would like to find a functional $g$ such that 

$$
g : L^2 \rightarrow \mathbb{R} \text{ such that } Y = g(X)
$$

** add scalar on function regression formula in lab4 **

Once the regression function is defined, it becomes easy to determine an approximate moisture level of a wheat grain sample from its spectral curve.

## The data

The *Moisturespectrum* data included in the `R` package `fds` is a data set that consists of near-infrared reflectance spectra of 100 wheat samples, measured in $2$ nm intervals from $1100$to $2500$nm. Their associated response variable, the samplesâ€™ moisture content which is a scalar, is included in a different dataset called *Moisturevalues*.

```{r, include=FALSE}
#-- load libraries
rm(list=ls())
library(fda)
library(refund)
library(tidyverse)
if (!require(fds)) install.packages("fds")
```

```{r data, fig.width=10, fig.height=4, fig.cap="\\label{fig:data}NIR spectroscopy of 100 wheat samples (on the left) and their moisture levels (on the right)"}
#-- load data and response variable
data("Moisturespectrum")
data("Moisturevalues")
Y = Moisturevalues
X = Moisturespectrum

#-- plot the data
par(mfrow=c(1,2))
plot(X, col = "grey", cex.lab = 0.7, cex.axis = 0.7)
plot(Y, xlab="Sample",
     ylab="Moisture level in %", 
     cex = 0.7,
     cex.lab = 0.7, 
     cex.axis = 0.7,
     pch=15)
```

```{r}
to_print = X$y[1:5, 1:5]
knitr::kable(to_print, align = 'c', 
             col.names = c("sample 1", "sample 2", "sample 3", "sample 4", "sample 5"),
             caption = "Table showing the first few rows and columns of the NIR spectra\\label{tab:sample}")
```

Figure \ref{fig:data} shows the spectral curves of 100 wheat samples densly packed together on the left, and their corresponding moisture level in percentage on the right. Table \ref{tab:sample} displays the first few rows and columns of the NIR spectra values. The row names of the matrix represent the wavelength in nano-meters and the column names represent the sample number.

# Approach and results

In this section, we will go through the details of our analysis steps we took in order to solve the problem. We start with an exploratory analysis, then move on to the modeling aspect along with the results and discussions. 

## Exploratory data analysis

As with all statistical modeling, data exploration is always the first step prior to any model development. Therefore, our analysis begins with a suitable exploratory analysis for functional data where we investigate a suitable basis function to smooth the data then compute the point wise statistics such as mean, standard deviation and their 95% confidence intervals.

### Manual smoothing

The first task is to smooth the functions using a basis. The data do not show any periodicity as we can see in Figure \ref{fig:data}. 
Therefore, the optimal basis function would be B splines. At first, we choose 25 basis and explore the plot produced in Figure \ref{fig:smooth}. We add to the latter the mean function (the black curve) as well as the standard deviation plotted in red. We also compute and plot the 95% confidence interval in dashed green.

```{r, include=FALSE}
mat = X$y
#dim(mat)

N = dim(mat)[1] # 701
M = dim(mat)[2] #100

#-- set number of basis and generate it
B25.basis = create.bspline.basis(rangeval = c(1,M), norder = 4, nbasis = 25)

#-- plot the generated basis
mat_fd = Data2fd(1:M,t(mat),B25.basis)
```

```{r smooth, fig.width=6, fig.height=4, fig.cap="\\label{fig:smooth}Smoothed curves using 25 B splines", fig.align='center'}
# Pointwise mean
muhat <- mean.fd(mat_fd)

# Standard deviation
sdhat <- sd.fd(mat_fd)

# Confidence interval 95 %
plot(mat_fd, lty = 1, ylim=c(0,1.5),col = "grey",cex.lab = 0.7,cex.axis = 0.7)
lines(muhat,lwd = 4, col = "black")
lines(muhat+1.96*sdhat,lwd=2,lty = 2,col = "green")
lines(muhat-1.96*sdhat,lwd=2,lty = 2,col = "green")
#lines(sdhat,lwd = 4, col = "red")
legend(84.5,1.55,c("mean","95% CI"), col = c("black","green"), lwd=4, cex = 0.7)
```

Interestingly, the smoothed data follows the classic two standard deviation rule surprisingly well with nearly all of the curves falling between the green lines. Figure \ref{fig:sd} show the point-wise sample standard deviation which gives us an idea about the variability of curves at any point $t$. Indeed, we notice more variability in the early samples than in the last samples. This may be explained by the fact that the moisture level in those samples are quite different.

```{r sd, fig.width=6, fig.height=4, fig.cap="\\label{fig:sd}Standard deviation of the curves", fig.align='center'}
plot(sdhat,lwd = 4, col = "red",cex.lab = 0.7,cex.axis = 0.7)
```

The point-wise sample standard deviation gives no information on how the values of the curves at point $t$ relate to those at point $s$. Therefore we compute the sample covariance function $\hat{c}(t,s)$ and we plot its perspective and its contour plot as seen in Figure \ref{fig:cov} and Figure \ref{fig:cont}

```{r cov, fig.width=7, fig.height=5, fig.cap="\\label{fig:cov}Perspective plot of the covariance function", fig.align='center'}
Chat <- var.fd(mat_fd)
#class(Chat) # we check that it's a bivariate functional data object.
eval_times<-seq(1,100,length=90)
Chat_matrix<-eval.bifd(eval_times,eval_times,Chat)

persp(eval_times,eval_times,Chat_matrix, theta = -45,phi =30, ticktype = "detailed", expand = 0.5, cex.lab = 0.7,cex.axis = 0.7)
```

```{r cont, fig.width=5, fig.height=5, fig.cap="\\label{fig:cont}Contour plot of the covariance function", fig.align='center'}
library(plotrix)
contour(eval_times, eval_times, Chat_matrix, lwd=2, cex.lab = 0.7,cex.axis = 0.7)
draw.circle(10,10,10, border = "red", lty = 3, lwd = 4)
?draw.circle
```

The 3D perspective plot shows that the variation in the spectrum is higher in the first samples. Indeed, this is confirmed in the contour plot in Figure \ref{fig:cont} in the highlighted red circle.

### Penalized smoothing

Typically, when the raw data curves exhibit a substantial level of noise, the functional objects constructed using manual basis expansion smoothing (i.e $M=25$ in our case) will inherit this variability, and thus resulting in *wiggly* curves. To avoid amplifying said variability, we will perform smoothing using a penalized approach.  

**Maybe add a formula that explaines PSS**

To choose the tuning parameter $\lambda$, generalized cross-validation (GCV) is employed. The aim is to minimize the penalized sum of squares with respect to the tuning parameter $\lambda$.

```{r gcv, fig.width=6, fig.height=4, fig.cap="\\label{fig:gcv}Values of the tuning parameter using generalized cross validation", fig.align='center'}
pen_basis = create.bspline.basis(c(1,M),nbasis=150,norder=4)
loglam = seq(-1,4,by=0.2)
nlam = length(loglam)

dfsave = numeric(nlam); names(dfsave) = loglam
gcvsave = numeric(nlam); names(gcvsave) = loglam

for (ilam in 1:nlam) {
	#cat(paste('log10 lambda =',loglam[ilam],'\n'))
	lambda = 10^loglam[ilam]
	fdParobj = fdPar(fdobj = pen_basis, Lfdobj = int2Lfd(2), lambda=lambda)
	smoothlist = smooth.basis(y=t(mat),fdParobj=fdParobj) ### A VOIR AVEC BASSEL
	dfsave[ilam] = smoothlist$df
	gcvsave[ilam] = sum(smoothlist$gcv) 
}

plot(loglam,gcvsave,type='b', lwd=2, 
     ylab = "generalized cross validation error", 
     xlab = "log lambda", cex.lab = 0.7,
     cex.axis = 0.7, cex = 0.7)
```

Generaized cross validation shows an optimal smoothing parameter $\lambda = 39.8$ when choosing a range for $\lambda$ between $[10^{-1}, 10^4]$ and a number of basis of $150$ knowing that we only have $100$ samples. The cross validation curve with respect to the values of $\lambda$ is shown in Figure \ref{fig:gcv}. Using the minimum value of $\lambda$, we penalize the basis expansion and repeat the smoothing task which yields the curves in the bottom plot in Figure \ref{fig:pen}.

```{r pen, fig.width=6, fig.height=8, fig.cap="\\label{fig:pen}Smoothed curves without penalization (top) vs with penalization (bottom)", fig.align='center'}
optilambda<-10^(loglam[which.min(gcvsave)])
fdParobj = fdPar(fdobj = pen_basis, Lfdobj = int2Lfd(2),lambda=optilambda)
result.fd = smooth.basis(y=t(mat),fdParobj=fdParobj)$fd

# Pointwise mean
mean.result.fd = mean.fd(result.fd)

# Standard deviation
sd.result.fd = std.fd(result.fd)


par(mfrow = c(2,1))

#-- plot with penalization
plot(result.fd, col = "grey", lty = 1, cex.lab = 0.7, cex.axis = 0.7, ylim = c(0.3, 1.25))
lines(mean.result.fd,lwd = 4, col = "black")
lines(mean.result.fd+1.96*sd.result.fd,lwd=2,lty = 2,col = "green")
lines(mean.result.fd-1.96*sd.result.fd,lwd=2,lty = 2,col = "green")
legend(84,1.27,c("mean","95% CI"), col = c("black","green"), lwd=3, cex = 0.7)


#-- plot without penalization
plot(mat_fd, lty = 1, ylim = c(0.3, 1.25),col = "grey",cex.lab = 0.7,cex.axis = 0.7)
lines(muhat,lwd = 4, col = "black")
lines(muhat+1.96*sdhat,lwd=2,lty = 2,col = "green")
lines(muhat-1.96*sdhat,lwd=2,lty = 2,col = "green")
#lines(sdhat,lwd = 4, col = "red")
legend(84.5,1.55,c("mean","95% CI"), col = c("black","green"), lwd=4, cex = 0.7)
```

Indeed, the results indicate that penalizing the smoothed basis reduces unnecessary variance significantly. The mean curve of the penalized smoothing seen in the top plot of Figure \ref{fig:pen} is much less noisy, showing peaks where the relative difference between the curves actually matters, and a smooth plateau where the difference is insignificant.

### Functional principal components analysis

One of the most useful tools in functional data analysis is the principal component analysis. Estimated functional principal components, EFPCâ€™s, are related to the sample covariance function $\hat{c}(t,s)$. Similar to usual multivariate statistics, we try to reduce the dimensionality of the feature space using only a few functions $\hat{\nu}_j$ such that the centered functions $X_n - \bar{X}_N$ are represented as follows :

$$
X_n(t) - \bar{X}_N(t) \approx \sum_{j=1}^p \xi_{nj} \hat{\nu}_j(t)
$$

Where $p$ is a much smaller dimension than $M$ the number of basis. Note that $\xi_{nj}$ are the scores of the components. The principal component analysis will be applied on the functions produced after penalized smoothing observed in the top plot of Figure \ref{fig:pen}.

```{r pca, fig.align="center", fig.width=6, fig.height=4, fig.cap="\\label{fig:pen}The percentage of variability explained vs FPCs on smoothed curves using penalization"}
result.fd.PCA = pca.fd(fdobj = result.fd, nharm = 5)
result.fd.PCA$varprop
mat_fd.PCA = pca.fd(fdobj = mat_fd, nharm = 5)

#-- get cumulative variability
c = cumsum(result.fd.PCA$varprop)
plot(c, type = "b", lwd = 2, ylim = c(0.999,1),
     col = "steelblue", 
     xlab = "Number of principal components", 
     ylab = "% of variability explained",
     cex = 0.7, cex.lab = 0.7, cex.axis = 0.7)
```

```{r}
to_print = t(c)
row.names(to_print) = "Cumulated Var."
knitr::kable(to_print, col.names = c("FPC1", "FPC2", "FPC3", "FPC4", "FPC5"), align = 'c', caption = "Table showing the cumulated variance of the first 5 PC\\label{tab:pctab}")
```

Figure \ref{fig:pca} and Table \ref{tab:pctab} indicate that we are able to capture more than $99$% of the variability using only the first principal component. This is not surprising due to the fact that the plots are very similar and densly packed together as we can observe in the Figure \ref{fig:data}. 

```{r harm, fig.align="center", fig.width=6, fig.height=4, fig.cap="\\label{fig:harm}plot of the first three FPCs"}
plot(result.fd.PCA$harmonics[1:3], lwd =2 , cex.lab = 0.7, cex.axis = 0.7)
```

We visualize the plot of the first 3 components in Figure \ref{fig:harm}. The first principal component (in black line) indicates that almost all samples' variance is captured equivalently. This is coherent with our pevious findings regarding curve similarities. Further exploratory analysis regarding curve registration is not necesarry, and therefore will not be explored in this study.

## Modeling

As we can see in Figure \ref{fig:data}, the response variable is a scalar (i.e. a number on $\mathbb{R}$) which makes a scalar-on-function regression analysis an ideal candidate to solve the problem at hand.

TO DO Modeling
Maybe a test train split ? (bonus)
Scalar on function regression
Evaluate model R2

Maybe scalar on function regression using FPCA (for later)

Modeling without FPCA
modeling with FPCA

# Conclusion

# References

[ref.1] Grain moisture â€“ guidelines for measurement
link https://projectblue.blob.core.windows.net/media/Default/Imported%20Publication%20Docs/Grain%20moisture%20%E2%80%93%20guidelines%20for%20measurement.pdf

[ref.2] Prediction of wheat quality parameters using near-infrared spectroscopy and artificial neural networks
link https://www.researchgate.net/publication/227299821_Prediction_of_wheat_quality_parameters_using_near-infrared_spectroscopy_and_artificial_neural_networks
